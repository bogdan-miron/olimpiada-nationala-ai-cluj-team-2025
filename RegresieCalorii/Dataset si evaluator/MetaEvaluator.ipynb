{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "eval_df = pd.read_csv('../Solutii/rezultate.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "\n",
    "def evaluate_candidate_output(ground_truth_path:str, \n",
    "                              candidate_name: str, \n",
    "                              candidate_output_path: str) -> Tuple[float, List]:\n",
    "    detailed_results = [] \n",
    "    \n",
    "    # Load answer files\n",
    "    try:\n",
    "        output_0 = pd.read_csv(os.path.join(candidate_output_path, 'output_0.csv'))\n",
    "        output_1 = pd.read_csv(os.path.join(candidate_output_path, 'output_1.csv'))\n",
    "        output_2 = pd.read_csv(os.path.join(candidate_output_path, 'output_2.csv'))\n",
    "    except:\n",
    "        print(\"Candidat nu are toate fisierele: \", candidate_name)\n",
    "        return 0, [0, 0, 0, 0, 0, 0]\n",
    "\n",
    "    #### Evaluate the output_1 file ####\n",
    "    # print(f\"Evaluating candidate: {candidate_name}\")\n",
    "\n",
    "    # Check if the output_0 has the expected columns # 'Samples', 'No. Males', 'Average Duration', 'SeniorUsers'\n",
    "    is_output_0_correct = output_0.columns.tolist() == ['Samples', 'No. Males', 'Average Duration', 'SeniorUsers']\n",
    "    score_output_0 = 0\n",
    "    if is_output_0_correct:\n",
    "        # print(\"output_0 - columns are correct\")\n",
    "        # print(\"restul1\", output_0[\"Samples\"].iloc[0])\n",
    "        if output_0[\"Samples\"].iloc[0] == 9000:\n",
    "            detailed_results.append(10)\n",
    "            score_output_0 += detailed_results[-1]\n",
    "            # print(\"Samples column is correct\")\n",
    "\n",
    "        if output_0[\"No. Males\"].iloc[0] == 4443:\n",
    "            detailed_results.append(10)\n",
    "            score_output_0 += detailed_results[-1]\n",
    "            # print(\"No. Males column is correct\")\n",
    "\n",
    "        if output_0[\"Average Duration\"].iloc[0] == 15.51:\n",
    "            detailed_results.append(10)\n",
    "            score_output_0 += detailed_results[-1]\n",
    "            # print(\"Average Duration column is correct\")\n",
    "\n",
    "        if output_0[\"SeniorUsers\"].iloc[0] == 412:\n",
    "            detailed_results.append(10)\n",
    "            score_output_0 += detailed_results[-1]\n",
    "            # print(\"SeniorUsers column is correct\")\n",
    "    else:\n",
    "            detailed_results.extend([0]*4)\n",
    "\n",
    "    ### Evaluate the output_1 file ####\n",
    "    score_output_1 = 0\n",
    "    is_output_1_correct = output_1.columns.tolist() == [\"Calories\"]\n",
    "    if is_output_1_correct:\n",
    "        # print(\"output_1 columns are correct\")\n",
    "\n",
    "        # Load the dataset_eval_t dataset that contains the column Calories then compare it with the output_2 price column using MAE metric\n",
    "        # Do the mae for each row then sum all the mae and divide it by the number of rows\n",
    "        dataset_eval_t = pd.read_csv(os.path.join(ground_truth_path, 'task1_dataset_eval_t.csv'))\n",
    "\n",
    "        mae_task1 = (dataset_eval_t[\"Calories\"] - output_1[\"Calories\"]).abs().sum() / len(dataset_eval_t)\n",
    "        # print(\"MAE: \", mae)\n",
    "\n",
    "        # Assign scores based on MAE value\n",
    "        import bisect\n",
    "        ranges = [8.5, 9, 10, 12]\n",
    "        scores = [40, 30, 20, 10]\n",
    "\n",
    "        index = bisect.bisect_left(ranges, mae_task1)\n",
    "        if index < len(scores):\n",
    "            score_output_1 += scores[index]\n",
    "\n",
    "        detailed_results.append(score_output_1)\n",
    "\n",
    "        # print(\"Took the score: \", score_output_1)\n",
    "    else:\n",
    "        detailed_results.append(0)\n",
    "\n",
    "    score_output_2 = 0\n",
    "    is_output_2_correct = output_2.columns.tolist() == [\"Calories\"]\n",
    "    if is_output_2_correct:\n",
    "        # print(\"output_2 columns are correct\")\n",
    "\n",
    "        # Load the dataset_eval_t dataset that contains the column Calories then compare it with the output_2 price column using MAE metric\n",
    "        # Do the mae for each row then sum all the mae and divide it by the number of rows\n",
    "        dataset_eval_t = pd.read_csv(os.path.join(ground_truth_path, 'task2_dataset_eval_t.csv'))\n",
    "\n",
    "        mae_task2 = (dataset_eval_t[\"Calories\"] - output_2[\"Calories\"]).abs().sum() / len(dataset_eval_t)\n",
    "        # print(\"MAE: \", mae)\n",
    "\n",
    "        # Assign scores based on MAE value\n",
    "        import bisect\n",
    "        ranges = [18, 23, 26, 30]\n",
    "        scores = [20, 15, 10, 5]\n",
    "\n",
    "        index = bisect.bisect_left(ranges, mae_task2)\n",
    "        if index < len(scores):\n",
    "            score_output_2 += scores[index]\n",
    "\n",
    "        detailed_results.append(score_output_2)\n",
    "\n",
    "        # print(\"Took the score: \", score_output_2)\n",
    "    else:\n",
    "        detailed_results.append(0)\n",
    "\n",
    "\n",
    "    # Calculate the total score\n",
    "    total_score = score_output_0 + score_output_1 + score_output_2\n",
    "    # print(\"Total score: \", total_score)\n",
    "\n",
    "    return score_output_0, mae_task1, mae_task2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Total'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_4580\\1263808769.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m     63\u001b[0m                 \u001b[1;34m'lr'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m             \u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcandidate_results\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mby\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"Total\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mascending\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Sort in descending order (highest score first)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"rezultate_indepth.csv\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Mano\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, by, axis, ascending, inplace, kind, na_position, ignore_index, key)\u001b[0m\n\u001b[0;32m   7185\u001b[0m             \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7186\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mby\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7187\u001b[0m             \u001b[1;31m# len(by) == 1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7188\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 7189\u001b[1;33m             \u001b[0mk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_label_or_level_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mby\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   7190\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7191\u001b[0m             \u001b[1;31m# need to rewrap column in Series to apply key function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7192\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Mano\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1907\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mother_axes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1908\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_level_reference\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1909\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_level_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1910\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1911\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1912\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1913\u001b[0m         \u001b[1;31m# Check for duplicates\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1914\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Total'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import shutil\n",
    "\n",
    "candidates_path = Path(\"../Solutii\")\n",
    "\n",
    "candidate_results = []\n",
    "\n",
    "for index, row in eval_df.iterrows():\n",
    "    if row['Total'] > 80:\n",
    "        candidat_name = row['Name']\n",
    "\n",
    "        candidat_path = candidates_path / candidat_name\n",
    "\n",
    "        relative_path = str(Path(candidat_path))\n",
    "        score_output_0, mae_task1, mae_task2 = evaluate_candidate_output(ground_truth_path=\"Dataset\",\n",
    "                                candidate_name=candidat_name,\n",
    "                                candidate_output_path=relative_path)\n",
    "        \n",
    "        if candidat_name not in ['Candidat_avansat', 'Candidat_incepator', 'Candiadat_ChatGpt']:\n",
    "        \n",
    "            json_path = os.path.join(candidat_path, 'config.json')\n",
    "            with open(json_path, \"r\") as file:\n",
    "                experiment_config = json.load(file) \n",
    "\n",
    "            new_json_path = os.path.join(candidat_path, candidat_name + '.json')\n",
    "\n",
    "            shutil.copy(json_path, new_json_path)\n",
    "\n",
    "            candidate_no = experiment_config['candidate_no']\n",
    "            DROP_COLUMNS = experiment_config['DROP_COLUMNS']\n",
    "            NORMALIZATIONS = experiment_config['NORMALIZATIONS']\n",
    "            HIDDEN_LAYERS = experiment_config['HIDDEN_LAYERS']\n",
    "            iterations = experiment_config['iterations']\n",
    "            solver = experiment_config['solver']\n",
    "            lr_init = experiment_config['lr_init']\n",
    "            momentum = experiment_config['momentum'] if solver == 'sgd' else None\n",
    "            lr = experiment_config['lr']\n",
    "        \n",
    "        else:\n",
    "            candidate_no = 'NA' # experiment_config['candidate_no']\n",
    "            DROP_COLUMNS = 'NA'# experiment_config['DROP_COLUMNS']\n",
    "            NORMALIZATIONS = 'NA'# experiment_config['NORMALIZATIONS']\n",
    "            HIDDEN_LAYERS = 'NA' # experiment_config['HIDDEN_LAYERS']\n",
    "            iterations ='NA' # experiment_config['iterations']\n",
    "            solver ='NA' # experiment_config['solver']\n",
    "            lr_init = 'NA'# experiment_config['lr_init']\n",
    "            momentum = 'NA'# experiment_config['momentum'] if solver == 'sgd' else None\n",
    "            lr = 'NA'# experiment_config['lr']\n",
    "\n",
    "        candidate_results.append({\n",
    "                'Name': candidat_name,\n",
    "                'total_score': row['Total'] ,\n",
    "                'mae_task1': mae_task1,\n",
    "                'mae_task2': mae_task2,\n",
    "                'candidate_no': candidate_no,\n",
    "                'DROP_COLUMNS': DROP_COLUMNS,\n",
    "                'NORMALIZATIONS': NORMALIZATIONS,\n",
    "                'HIDDEN_LAYERS': HIDDEN_LAYERS,\n",
    "                'iterations': iterations,\n",
    "                'solver': solver,\n",
    "                'lr_init': lr_init,\n",
    "                'momentum': momentum,\n",
    "                'lr': lr\n",
    "            })\n",
    "\n",
    "df = pd.DataFrame(candidate_results)\n",
    "df = df.sort_values(by=\"total_score\", ascending=False)  # Sort in descending order (highest score first)\n",
    "df.to_csv(\"rezultate_indepth.csv\", index=False)\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
